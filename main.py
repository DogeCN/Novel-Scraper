import re
import os
import asyncio
from typing import List, Dict
import aiohttp
from bs4 import BeautifulSoup
import readchar
from rich.console import Console

console = Console()


class InteractiveMenu:
    def __init__(self):
        self.items = []
        self.selected_index = 0

    def add_item(self, label: str, value):
        self.items.append({"label": label, "value": value})

    def render(self):
        console.clear()
        for i, item in enumerate(self.items):
            prefix = "â†’ " if i == self.selected_index else "  "
            style = "bold white on blue" if i == self.selected_index else "white"
            console.print(f"{prefix}{item['label']}", style=style)
        console.print("\n[dim]ä½¿ç”¨ â†‘ â†“ æ–¹å‘é”®é€‰æ‹©,å›è½¦ç¡®è®¤,ESC è¿”å›[/dim]")

    def show(self):
        if not self.items:
            return None
        self.selected_index = 0
        while True:
            self.render()
            key = readchar.readkey()
            if key == readchar.key.UP:
                self.selected_index = (self.selected_index - 1) % len(self.items)
            elif key == readchar.key.DOWN:
                self.selected_index = (self.selected_index + 1) % len(self.items)
            elif key == readchar.key.ENTER or key == "\r":
                return self.items[self.selected_index]
            elif key == readchar.key.ESC:
                return None


class SearchResultSelector:
    @staticmethod
    def show_search_results(results: List[Dict], keyword: str):
        if not results:
            console.print("[red]âŒ æœªæ‰¾åˆ°æœç´¢ç»“æœ[/red]")
            return None
        selected_index = 0
        while True:
            console.clear()
            for i, result in enumerate(results):
                prefix = f"[{i+1}] "
                style = "bold white on blue" if i == selected_index else "white"
                console.print(f"{prefix}{result['title']}", style=style)
            result = results[selected_index]
            console.print("\n[bold]è¯¦ç»†ä¿¡æ¯ï¼š[/bold]")
            console.print(f"  [dim]ID:[/dim] {result['id']}")
            console.print(f"  [yellow]ä½œè€…: {result['author']}[/yellow]")
            console.print(f"  [green]çŠ¶æ€: {result['status']}[/green]")
            console.print(f"  [cyan]å­—æ•°: {result['word_count']}[/cyan]")
            if result["description"]:
                desc_preview = result["description"]
                console.print(f"  [dim]ç®€ä»‹: {desc_preview}[/dim]")
            console.print("\n[dim]ä½¿ç”¨ â†‘ â†“ æ–¹å‘é”®é€‰æ‹©,å›è½¦ç¡®è®¤,ESC è¿”å›[/dim]")
            key = readchar.readkey()
            if key == readchar.key.UP:
                selected_index = (selected_index - 1) % len(results)
            elif key == readchar.key.DOWN:
                selected_index = (selected_index + 1) % len(results)
            elif key == readchar.key.ENTER or key == "\r":
                return results[selected_index]
            elif key == readchar.key.ESC:
                return None


class ChapterRangeSelector:
    @staticmethod
    def show_range_selector(total_chapters: int):
        console.clear()
        menu = InteractiveMenu()
        menu.add_item("çˆ¬å–å…¨éƒ¨ç« èŠ‚", ("all", 1, total_chapters))
        menu.add_item("çˆ¬å–æŒ‡å®šç« èŠ‚èŒƒå›´", ("custom", None, None))
        menu.add_item("é€€å‡º", ("exit", None, None))
        choice = menu.show()
        if not choice or choice["value"][0] == "exit":
            return None, None
        mode, start, end = choice["value"]
        if mode == "all":
            return start, end
        console.clear()
        console.print("[cyan]è‡ªå®šä¹‰ç« èŠ‚èŒƒå›´[/cyan]\n")
        try:
            start_chapter = int(input(f"è¯·è¾“å…¥èµ·å§‹ç« èŠ‚ (1-{total_chapters}): ").strip())
            if start_chapter < 1 or start_chapter > total_chapters:
                console.print("[red]âŒ èµ·å§‹ç« èŠ‚æ— æ•ˆ[/red]")
                return None, None
            end_chapter = input(
                f"è¯·è¾“å…¥ç»“æŸç« èŠ‚ ({start_chapter}-{total_chapters}) [é»˜è®¤å…¨éƒ¨]: "
            ).strip()
            if end_chapter:
                end_chapter = int(end_chapter)
                if end_chapter < start_chapter or end_chapter > total_chapters:
                    console.print("[red]âŒ ç»“æŸç« èŠ‚æ— æ•ˆ[/red]")
                    return None, None
            else:
                end_chapter = total_chapters
            return start_chapter, end_chapter
        except ValueError:
            console.print("[red]âŒ è¯·è¾“å…¥æœ‰æ•ˆçš„æ•°å­—[/red]")
            return None, None


class ConfirmationDialog:
    @staticmethod
    def show_confirmation(message: str) -> bool:
        menu = InteractiveMenu()
        menu.add_item("ç¡®è®¤ (Y)", True)
        menu.add_item("å–æ¶ˆ (N)", False)
        console.print(f"[yellow]{message}[/yellow]\n")
        choice = menu.show()
        return choice["value"] if choice else False


class ProgressBar:
    @staticmethod
    def show_success(message: str):
        console.print(f"[green]âœ“ {message}[/green]")

    @staticmethod
    def show_error(message: str):
        console.print(f"[red]âœ— {message}[/red]")


def show_main_menu():
    menu = InteractiveMenu()
    menu.add_item("ğŸ“š æœç´¢å°è¯´", "search")
    menu.add_item("ğŸ”¢ ç›´æ¥è¾“å…¥å°è¯´ID", "input")
    menu.add_item("âŒ é€€å‡º", "exit")
    choice = menu.show()
    return choice["value"] if choice else None


base_url_template = "https://www.xpxs.net/index/{}"
chapter_template = "https://www.xpxs.net/chapter/{}"
headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
    "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
}


async def fetch_page_async(
    url: str, session: aiohttp.ClientSession, retry: int = 3
) -> str:
    for attempt in range(retry):
        try:
            async with session.get(
                url, headers=headers, timeout=aiohttp.ClientTimeout(total=15)
            ) as response:
                response.raise_for_status()
                text = await response.text(encoding="utf-8", errors="ignore")
                return text
        except Exception as e:
            if attempt < retry - 1:
                console.print(f"[red]è·å–é¡µé¢å¤±è´¥: {url}, é”™è¯¯: {e}, æ­£åœ¨é‡è¯•...[/red]")
                await asyncio.sleep(0.1)
            else:
                console.print(f"[red]è·å–é¡µé¢å¤±è´¥: {url}, é”™è¯¯: {e}[/red]")
                raise


def extract_page_info(html: str, novel_id: str):
    soup = BeautifulSoup(html, "html.parser")
    pages = []
    seen_urls = set()
    select_element = soup.find("select", {"id": "indexselect"})
    if select_element:
        options = select_element.find_all("option")
        for idx, option in enumerate(options, 1):
            value = option.get("value", "").strip()
            text = option.text.strip()
            match = re.search(r"(\d+)\s*-\s*(\d+)\s*ç« ", text)
            if match and value:
                start_chapter = int(match.group(1))
                end_chapter = int(match.group(2))
                full_url = f"https://www.xpxs.net{value}"
                if full_url not in seen_urls:
                    seen_urls.add(full_url)
                    pages.append(
                        {
                            "index": idx,
                            "start_chapter": start_chapter,
                            "end_chapter": end_chapter,
                            "url": full_url,
                            "text": text,
                        }
                    )
    title_element = soup.find("title")
    novel_title = title_element.text.strip() if title_element else f"Novel_{novel_id}"
    title_match = re.search(r"^(.*?)æœ€æ–°ç« èŠ‚ç›®å½•", novel_title)
    if title_match:
        novel_title = title_match.group(1).strip()
    return pages, novel_title


def parse_chapter_list(html: str, page_start_index: int):
    soup = BeautifulSoup(html, "html.parser")
    chapters = []
    links = soup.find_all("a", href=re.compile(r"/chapter/"))
    for idx, link in enumerate(links):
        title = link.text.strip()
        href = link.get("href", "").strip()
        if title and href:
            full_url = (
                href if href.startswith("http") else f"https://www.xpxs.net{href}"
            )
            chapters.append(
                {"index": page_start_index + idx, "title": title, "url": full_url}
            )
    return chapters


async def search_novels(keyword: str, session: aiohttp.ClientSession):
    search_url = f"https://www.xpxs.net/search/?searchkey={keyword}"
    try:
        html = await fetch_page_async(search_url, session)
        soup = BeautifulSoup(html, "html.parser")
        results = []
        dl_elements = soup.find_all("dl")
        for dl in dl_elements:
            title_link = dl.find("dt")
            if not title_link:
                continue
            title_a = title_link.find("a")
            if not title_a:
                continue
            title = title_a.get("title", "").strip() or title_a.text.strip()
            href = title_a.get("href", "").strip()
            novel_id_match = re.search(r"/book/(\d+)/", href)
            if not novel_id_match:
                continue
            novel_id = novel_id_match.group(1)
            dd_elements = dl.find_all("dd")
            description = ""
            author = ""
            status = ""
            word_count = ""
            for dd in dd_elements:
                text = dd.get_text(strip=True)
                author_a = dd.find("a")
                if author_a and (
                    author_a.get("href", "").startswith("/author/")
                    or "è¿è½½" in text
                    or "å…¨æœ¬" in text
                ):
                    author = author_a.text.strip()
                    spans = dd.find_all("span")
                    if len(spans) >= 1:
                        status = spans[0].text.strip()
                    if len(spans) >= 2:
                        word_count = spans[1].text.strip()
                elif text and len(text) > 20:
                    description = text
            results.append(
                {
                    "id": novel_id,
                    "title": title,
                    "author": author,
                    "status": status,
                    "word_count": word_count,
                    "description": description,
                }
            )
        return results
    except Exception as e:
        console.print(f"[red]âŒ æœç´¢å¤±è´¥: {e}[/red]")
        return []


def clean_content(text: str, chapter_title: str) -> str:
    cleaned = text.replace("\r\n", "\n").replace("\r", "\n")
    cleaned = re.sub(r"<[^>]*>", "", cleaned)
    # éœ€è¦æ‰¹é‡å‰”é™¤çš„å†…å®¹æ­£åˆ™
    patterns = [
        r"è™¾çš®å°è¯´ã€.*?ã€‘.*?æœ€æ–°ç« èŠ‚[ã€‚]?",
        r"æœ¬ç« æœªå®Œ.*?ä¸‹ä¸€é¡µ.*?é˜…è¯»[ã€‚]?",
        r"ç« èŠ‚æŠ¥é”™.*?å…ç™»å½•[ã€‚]?",
        r".*?è¯·å¤§å®¶æ”¶è—.*?æ›´æ–°é€Ÿåº¦.*?æœ€å¿«[ã€‚]?",
        r".*?åé¢è¿˜æœ‰å“¦.*?åé¢æ›´ç²¾å½©.*?",
    ]
    for pat in patterns:
        cleaned = re.sub(pat, "", cleaned)
    title_pattern = rf"^\s*{re.escape(chapter_title)}\s*"
    cleaned = re.sub(title_pattern, "", cleaned, flags=re.MULTILINE)
    cleaned = re.sub(r"^\s+|\s+$", "", cleaned, flags=re.MULTILINE)
    cleaned = re.sub(r"[ \t]+", " ", cleaned)
    cleaned = re.sub(r"^\s*[â”€=\*]{5,}\s*$", "", cleaned, flags=re.MULTILINE)
    return cleaned.strip()


async def parse_chapter_content(
    chapter_index: int,
    chapter_title: str,
    chapter_url: str,
    session: aiohttp.ClientSession,
    retry: int = 3,
) -> str:
    all_content = []
    current_page = chapter_url
    page_number = 1
    for attempt in range(retry):
        try:
            while current_page:
                html = await fetch_page_async(current_page, session)
                soup = BeautifulSoup(html, "html.parser")
                content_selectors = [
                    "#booktxt",
                    ".content",
                    "#content",
                    ".chapter-content",
                    ".novel-content",
                ]
                page_content = ""
                for selector in content_selectors:
                    element = soup.select_one(selector)
                    if element:
                        page_content = element.get_text()
                        break
                if page_content:
                    cleaned = clean_content(page_content, chapter_title)
                    if cleaned:
                        all_content.append(cleaned)
                next_button = soup.find("a", {"rel": "next"})
                if next_button:
                    next_href = next_button.get("href", "")
                    if next_href and re.search(r"_\d+\.html$", next_href):
                        current_page = (
                            f"https://www.xpxs.net{next_href}"
                            if next_href.startswith("/")
                            else next_href
                        )
                        page_number += 1
                        await asyncio.sleep(0.01)
                    else:
                        current_page = None
                else:
                    current_page = None
            break
        except Exception as e:
            if attempt < retry - 1:
                console.print(f"\tç¬¬ {page_number} é¡µè·å–å¤±è´¥, æ­£åœ¨é‡è¯•... ({e})")
                await asyncio.sleep(0.1)
            else:
                console.print(f"\tç¬¬ {page_number} é¡µè·å–å¤±è´¥: {e}")
                break
    combined_content = "\n\n".join(all_content)
    chapter_title = re.sub(r"ç¬¬\s*\d+\s*[ç« èŠ‚å›]\s*", "", chapter_title)
    formatted_title = f"ç¬¬ {chapter_index} ç«  {chapter_title}"
    console.print(f"{formatted_title} - è·å–å®Œæˆ, å…± {len(combined_content)} å­—ç¬¦")
    return f"{formatted_title}\n\n{combined_content}"


async def get_pages_and_title(novel_id: str, session: aiohttp.ClientSession):
    base_url = base_url_template.format(novel_id)
    console.print("[yellow]æ­£åœ¨è·å–åˆ†é¡µä¿¡æ¯...[/yellow]\n")
    first_page_html = await fetch_page_async(f"{base_url}/", session)
    pages, novel_title = extract_page_info(first_page_html, novel_id)
    if not pages:
        console.print("[red]âŒ æœªæ‰¾åˆ°åˆ†é¡µä¿¡æ¯,è¯·æ£€æŸ¥å°è¯´IDæ˜¯å¦æ­£ç¡®[/red]")
        return [], novel_title
    console.print(f"æ‰¾åˆ° {len(pages)} ä¸ªåˆ†é¡µï¼š")
    for page in pages:
        console.print(
            f"\tç¬¬{page['index']}é¡µ: {page['start_chapter']} - {page['end_chapter']}ç« "
        )
    console.print()
    return pages, novel_title


async def scrape_novel(
    start_chapter: int,
    end_chapter: int,
    pages,
    session: aiohttp.ClientSession,
):
    # å…ˆè§£ææ‰€æœ‰åˆ†é¡µé¡µé¢ï¼Œè·å–çœŸå®ç« èŠ‚URLå’Œæ ‡é¢˜
    chapters_all = []

    async def fetch_page_chapters(page):
        try:
            html = await fetch_page_async(page["url"], session)
            return parse_chapter_list(html, page["start_chapter"])
        except Exception as e:
            console.print(f"[red]åˆ†é¡µè·å–å¤±è´¥: {page['url']} {e}[/red]")
            return []

    page_chapters_list = await asyncio.gather(
        *[fetch_page_chapters(page) for page in pages]
    )
    for page_chapters in page_chapters_list:
        chapters_all.extend(page_chapters)
    # æŒ‰ç”¨æˆ·é€‰æ‹©èŒƒå›´ç­›é€‰
    chapters_to_scrape = [
        c for c in chapters_all if start_chapter <= c["index"] <= end_chapter
    ]
    if not chapters_to_scrape:
        console.print("[red]âŒ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•ç« èŠ‚[/red]")
        return
    console.print(f"\n[yellow]å¼€å§‹çˆ¬å– {len(chapters_to_scrape)} ä¸ªç« èŠ‚...[/yellow]\n")
    output_dir = os.path.dirname(output_file)
    if output_dir and not os.path.exists(output_dir):
        os.makedirs(output_dir)
    results = await asyncio.gather(
        *[
            parse_chapter_content(chap["index"], chap["title"], chap["url"], session)
            for chap in chapters_to_scrape
        ]
    )
    with open(output_file, "w", encoding="utf-8") as f:
        success_count = 0
        fail_count = 0
        for i, content in enumerate(results):
            progress = f"[{i + 1}/{len(chapters_to_scrape)}]"
            if content.strip():
                f.write("\n\n" + content)
                f.flush()
                ProgressBar.show_success(f"{progress} å·²å†™å…¥æ–‡ä»¶ ({len(content)} å­—ç¬¦)")
                success_count += 1
            else:
                ProgressBar.show_error(f"{progress} å¤±è´¥: å†…å®¹ä¸ºç©º")
                fail_count += 1
    console.print()
    console.print(
        f"[bold green]âœ“ çˆ¬å–å®Œæˆ![/bold green]\n",
    )
    console.print(
        f"æˆåŠŸ: [green]{success_count}[/green] ç« \nå¤±è´¥: [red]{fail_count}[/red] ç« \næ–‡ä»¶: [cyan]{output_file}[/cyan]"
    )


async def main():
    async with aiohttp.ClientSession() as session:
        while True:
            choice = show_main_menu()
            if not choice or choice == "exit":
                console.print("[yellow]ğŸ‘‹ å†è§![/yellow]")
                break
            novel_id = novel_title = ""
            if choice == "search":
                console.print("\n[cyan]è¯·è¾“å…¥æœç´¢å…³é”®è¯:[/cyan] ", end="")
                keyword = input().strip()
                if not keyword:
                    console.print("[red]âŒ æœç´¢å…³é”®è¯ä¸èƒ½ä¸ºç©º![/red]")
                    continue
                results = await search_novels(keyword, session)
                if not results:
                    console.print("[red]âŒ æœªæ‰¾åˆ°ç›¸å…³å°è¯´[/red]")
                    readchar.readkey()
                    continue
                selected = SearchResultSelector.show_search_results(results, keyword)
                if not selected:
                    continue
                novel_id = selected["id"]
                novel_title = selected["title"]
            elif choice == "input":
                console.print("\n[cyan]è¯·è¾“å…¥å°è¯´ID:[/cyan] ", end="")
                novel_id = input().strip()
                if not novel_id:
                    console.print("[red]âŒ å°è¯´IDä¸èƒ½ä¸ºç©º![/red]")
                    continue
            pages, novel_title = await get_pages_and_title(novel_id, session)
            if not pages:
                console.print("[red]âŒ æ— æ³•è·å–ç« èŠ‚ä¿¡æ¯,è¯·æ£€æŸ¥å°è¯´ID[/red]")
                readchar.readkey()
                continue
            start_chapter, end_chapter = ChapterRangeSelector.show_range_selector(
                max(page["end_chapter"] for page in pages)
            )
            if not start_chapter or not end_chapter:
                continue
            global output_file
            output_file = f"novels/{re.sub(r'[<>:\"/\\|?*]', '', novel_title)}_{start_chapter}-{end_chapter}.txt"
            if not ConfirmationDialog.show_confirmation("æŒ‰å›è½¦ç¡®è®¤å¼€å§‹çˆ¬å–, ESC å–æ¶ˆ"):
                console.print("[yellow]å·²å–æ¶ˆ[/yellow]")
                continue
            console.print()
            await scrape_novel(start_chapter, end_chapter, pages, session)
            readchar.readkey()


if __name__ == "__main__":
    asyncio.run(main())
